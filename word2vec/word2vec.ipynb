{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e8919",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Machine learning is the study of computer algorithms that \\\n",
    "improve automatically through experience. It is seen as a \\\n",
    "subset of artificial intelligence. Machine learning algorithms \\\n",
    "build a mathematical model based on sample data, known as \\\n",
    "training data, in order to make predictions or decisions without \\\n",
    "being explicitly programmed to do so. Machine learning algorithms \\\n",
    "are used in a wide variety of applications, such as email filtering \\\n",
    "and computer vision, where it is difficult or infeasible to develop \\\n",
    "conventional algorithms to perform the needed tasks.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a767c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    pattern = re.compile(r\"[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*\")\n",
    "    return pattern.findall(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4933399",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a42430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(tokens):\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb9246b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'email': 0,\n",
       " 'sample': 1,\n",
       " 'needed': 2,\n",
       " 'mathematical': 3,\n",
       " 'variety': 4,\n",
       " 'improve': 5,\n",
       " 'computer': 6,\n",
       " 'algorithms': 7,\n",
       " 'build': 8,\n",
       " 'subset': 9,\n",
       " 'order': 10,\n",
       " 'make': 11,\n",
       " 'applications': 12,\n",
       " 'tasks': 13,\n",
       " 'in': 14,\n",
       " 'filtering': 15,\n",
       " 'based': 16,\n",
       " 'to': 17,\n",
       " 'do': 18,\n",
       " 'experience': 19,\n",
       " 'training': 20,\n",
       " 'it': 21,\n",
       " 'where': 22,\n",
       " 'the': 23,\n",
       " 'seen': 24,\n",
       " 'without': 25,\n",
       " 'wide': 26,\n",
       " 'is': 27,\n",
       " 'explicitly': 28,\n",
       " 'decisions': 29,\n",
       " 'predictions': 30,\n",
       " 'vision': 31,\n",
       " 'intelligence': 32,\n",
       " 'machine': 33,\n",
       " 'through': 34,\n",
       " 'are': 35,\n",
       " 'infeasible': 36,\n",
       " 'as': 37,\n",
       " 'of': 38,\n",
       " 'data': 39,\n",
       " 'on': 40,\n",
       " 'known': 41,\n",
       " 'learning': 42,\n",
       " 'or': 43,\n",
       " 'so': 44,\n",
       " 'difficult': 45,\n",
       " 'programmed': 46,\n",
       " 'artificial': 47,\n",
       " 'study': 48,\n",
       " 'conventional': 49,\n",
       " 'a': 50,\n",
       " 'used': 51,\n",
       " 'such': 52,\n",
       " 'develop': 53,\n",
       " 'model': 54,\n",
       " 'and': 55,\n",
       " 'that': 56,\n",
       " 'automatically': 57,\n",
       " 'perform': 58,\n",
       " 'being': 59}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id, id_to_word = mapping(tokens)\n",
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d919fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def concat(*iterables):\n",
    "    for iterable in iterables:\n",
    "        yield from iterable\n",
    "\n",
    "\n",
    "def one_hot_encode(id, vocab_size):\n",
    "    res = [0] * vocab_size\n",
    "    res[id] = 1\n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window):\n",
    "    X = []\n",
    "    y = []\n",
    "    n_tokens = len(tokens)\n",
    "\n",
    "    for i in range(n_tokens):\n",
    "        idx = concat(\n",
    "            range(max(0, i - window), i), range(i, min(n_tokens, i + window + 1))\n",
    "        )\n",
    "        for j in idx:\n",
    "            if i == j:\n",
    "                continue\n",
    "            X.append(one_hot_encode(word_to_id[tokens[i]], len(word_to_id)))\n",
    "            y.append(one_hot_encode(word_to_id[tokens[j]], len(word_to_id)))\n",
    "\n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ba2b804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330, 60) (330, 60)\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_training_data(tokens, word_to_id, 2)\n",
    "print(X.shape, y.shape)\n",
    "print(len(word_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87bf203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
